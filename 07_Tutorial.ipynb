{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a841297-5c56-4e64-a479-a2ccfb6f8072",
   "metadata": {},
   "source": [
    "UM MSBA - BGEN632\n",
    "\n",
    "# Week 7: Data Manipulation\n",
    "\n",
    "The purpose of this tutorial is to familiarize you with how to access data, import data, and manipulate that data into usable formats. This requires knowledge of data frames.\n",
    "\n",
    "To begin, we will go over the concept of a data frame. Then, we will dive in to how to import data into a data frame object, access the contents of the data frame by index values, sort the data, and several other functions and methods.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "To setup a notebook, we should first load required modules and set the working directory.\n",
    "\n",
    "Following best practice, we will place import statements for the libraries/modules/packages used in this tutorial at the beginning of the notebook. You will need to download and install the following packages using your preferred package manager if you have not already done so:\n",
    "\n",
    "* `pandas`\n",
    "* `numpy`\n",
    "* `scikit-learn`\n",
    "\n",
    "We will also cover the `os` module in the tutorial, but this is included in Python's standard library so you do not need to complete a separate installation. \n",
    "\n",
    "### Import Modules\n",
    "\n",
    "Run the code cell below to load required modules. Note that we are only importing the `KFold` class from `sklearn.model_selection` in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064c685-9d7c-4e51-b48b-802f950fc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1f28f-78b1-4a97-8e84-32c33192e710",
   "metadata": {},
   "source": [
    "### Set Working Directory\n",
    "\n",
    "Your working directory is the filepath where files are read into and saved out of Jupyter/Python. In other words, it is the location on your machine that Jupyter and Python assume is the starting place for all paths that you try to access or construct. The default working directory in a Jupyter Notebook file is the directory in which it is saved.\n",
    "\n",
    "To determine the current directory, we can use the `getcwd()` function from the `os` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a2032-61bf-4090-bfa0-34ea560de673",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()  # get current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6374c3-2e3c-47ef-bd6f-07e7be1f064f",
   "metadata": {},
   "source": [
    "Consider a hypothetical situation where you have a designated folder for your data science work. This folder contains your data and is also the location where you would like to save analysis results and visualizations. What to do? You could change your working directory to that location by using `os.chdir()` like so:\n",
    "\n",
    "```Python\n",
    "os.chdir()   # add your desired file path within the parentheses of this function to change the directory\n",
    "os.getcwd()  # then confirm the change by checking the current working directory\n",
    "```\n",
    "\n",
    "For this tutorial, your working directory will be set to the location of your Week 7 materials (i.e., the file path for your local copy of the Week 7 GitHub repo). Specifically, we will work in the data folder. Note that the working directory convention differs for Windows vs. Mac and Linux. My machine is a Mac so the code in my notebook looks like this:\n",
    "\n",
    "```Python\n",
    "os.chdir(\"/Users/obn/Documents/GitHub/UM-BGEN632/week7labs/data\")\n",
    "```\n",
    "\n",
    "Add your desired file path within the parentheses of the `os.chdir()` function in the code cell below and then run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb889c3a-85f0-40fb-ac81-846fec1ef9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir()   # change the directory\n",
    "os.getcwd()  # confirm the change "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25056632-2c0c-4f41-b7a7-7dea9adbe335",
   "metadata": {},
   "source": [
    "#### A Quick Aside: Defining a Path\n",
    "\n",
    "You can also define absolute or relative filepaths and use those filepaths as needed. A relative path is the path that is relative to the working directory location on your machine. An absolute path is a path that contains the entire path to the file or directory that you need to access. This path will begin at the home directory of your machine and will end with the file or directory that you wish to access. \n",
    "\n",
    "Absolute paths ensure that Python can find the exact file on your machine. However, machines can have a different path constructions, depending on the operating system, and contain usernames that are unique to that specific machine. There are ways to overcome this issue and others associated with finding files on different machines. The `os.path.join()` and `os.path.exists()` functions are particularly useful for finding files on different machines. [Check out this resource from the UVA Research Computing Portal to learn more](https://learning.rc.virginia.edu/courses/python-introduction/files/). \n",
    "\n",
    "---\n",
    "\n",
    "## Data Frame Overview\n",
    "\n",
    "Okay, let's talk about data. We can organize data into a data matrix (i.e., a table) where each row represents a thing observed and each column represents a variable. We might also refer to this as *tabular* data as the data is organized and displayed as a table. In the example below, some employee data for a fictional company with 63 employees is presented in a table. \n",
    "\n",
    "| Employee_ID | Tenure_Months | Leadership | Title | \n",
    "|:----|:---- |:---- |:---- |\n",
    "| e1 | 45 | $$True$$ | Project Manager |\n",
    "| e2 | 2 | $$False$$ | Junior Analyst |\n",
    "| e3 | 13 | $$False$$ | Adminstrative Assistant |\n",
    "| . | . | . | . |\n",
    "| . | . | . | . |\n",
    "| e63 | 19 | $$True$$ | Senior Engineer |\n",
    "\n",
    "\n",
    "### pandas DataFrame\n",
    "\n",
    "In Python, we can use the Python Data Analysis Library (pandas) to work with tabular data. This library provides the tools and data structures we need to effectively work with data. To do so, we need to load pandas in our notebook. Recall that we can simplify its name based on standard convention as we will use it over and over again.\n",
    "\n",
    "```Python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "In pandas, the primary data structure is a `DataFrame`. This is what we will use to represent and interact with our data. For those of you who have experience with *R Statistical Software*, you are likely already familiar with the concept of a data frame. \n",
    "\n",
    "So, what is it exactly? A pandas `DataFrame` is a 2-dimensional data structure. It can store data of different types (including characters, integers, floating point values, categorical, and more) in columns. Stated another way, it is a table with rows and columns. \n",
    "\n",
    "Let's return to the fictional employee data example provided above to highlight the qualities of a DataFrame:\n",
    "\n",
    "* Each row is a single *observation*: an employee.\n",
    "  * That means that the data in a row are *related* to each other.\n",
    "  * In this case, the values in a row all correspond to a single employee.\n",
    "* Each column is a single *variable*: employee ID, tenure, leadership, title.\n",
    "  * Columns are *named* (e.g., 'Tenure_Months' is the name of the second column).\n",
    "  * All columns within a DataFrame contain the same number of items and rows (63 in this example).\n",
    "  * Each variable has a specific data type (string, integer, boolean, categorical in this example, respectively).\n",
    "    * Note that while different columns may have different types, *each column must contain only one type of data*!\n",
    "    * In Python, you may encounter a `None` object which represents the absence of a value (or a null value) in a column.\n",
    "    * You may also encounter a `NaN` (Not a Number) object which represents undefined or nonsensical numerical values.\n",
    "    * [Here's a Medium article describing the difference between None and NaN](https://medium.com/data-and-beyond/none-nan-null-and-zero-in-python-ac326cfb73a2).\n",
    "\n",
    "These details ultimately represent metadata, or data about data:\n",
    "\n",
    "* Column names\n",
    "* Column data types\n",
    "* Length of columns | Number of rows\n",
    "* Column order\n",
    "* Row order\n",
    "\n",
    "\n",
    "#### Data Frame Differences: R vs. Python\n",
    "\n",
    "Data frames are built into base R. This means that the functions, commands, and other operations within R work very well with data frames. In contrast, data frames are provided by an external package (pandas) in Python. This means that data frames are not as well integrated in Python as in R. To help overcome this limitation, we can rely on data science libraries like NumPy and scikit-learn. Keep in mind that software updates may break the functionality of data frames. You may need to revert back to an older version of a library/module/package to regain functionality. I tend to prefer R for data manipulation, analysis, and visualization due to these and  other reasons. \n",
    "\n",
    "\n",
    "### From Spreadsheet to DataFrame\n",
    "\n",
    "If the fictional employee data table provided above were stored in a spreadsheet, we could import it in Python as a pandas DataFrame. For example, if the data were contained within a CSV file named 'employee_data.csv', we could import the data by using the pandas `read_csv()` function and store the data like so:\n",
    "\n",
    "```Python\n",
    "employees = pd.read_csv(\"employee_data.csv\")\n",
    "```\n",
    "\n",
    "This example also shows the use of our defined name `pd` for `pandas` functions! Also, while this example is based on a CSV file, pandas can support many file formats or data sources (csv, excel, sql, json, parquet, â€¦), each of them with the prefix read_*:\n",
    "\n",
    "\n",
    "```Python\n",
    "pd.read_csv(\"our_data.csv\")\n",
    "pd.read_excel(\"our_data.excel\")\n",
    "pd.read_sql(\"our_data.sql\")\n",
    "pd.read_json(\"our_data.json\")\n",
    "# etc.\n",
    "```\n",
    "\n",
    "### Load Data\n",
    "\n",
    "For this tutorial, we will start by working with the pine tree data in the Loblolly.csv file. [This dataset is from the R Datasets package](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/Loblolly) and is provided within the data folder of this week's repo. \n",
    "\n",
    "Run the code cell below to load and store the Loblolly dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f0ccb-1c0c-4109-9f65-00eb410aaec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly = pd.read_csv(\"Loblolly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66bc84-2622-40a8-ba17-9b6abffdc180",
   "metadata": {},
   "source": [
    "### Inspect Data \n",
    "\n",
    "After reading in data, you should **always** check it! You should never assume data is perfect and ready for analysis. You should inspect data for errors, inconsistencies, and missing values. You should also verify data types because this information will influence the accuracy and appropriateness of various operations applied to the data.\n",
    "\n",
    "Familiarizing yourself with the data in a DataFrame may entail selecting specific columns or rows, sorting data, and selecting data based on conditions. In this section, we will go over some approaches for inspecting data.\n",
    "\n",
    "#### Heads and Tails\n",
    "\n",
    "To quickly inspect the data, we can use the `head(n)` or `tail(n)` functions, where `n` is the number of lines that should be returned. Specifically, we can use `head(n)` to view the first `n` lines in a DataFrame and we can use `tail(n)` to view the last `n` lines in a DataFrame. Alternatively, you can display the first and last five rows of the DataFrame with just the DataFrame name.\n",
    "\n",
    "```Python\n",
    "loblolly.head()    # display the first five rows of the DataFrame (the default)\n",
    "loblolly.head(10)  # display the first 10 rows of the DataFrame\n",
    "loblolly.head(50)  # display the first 50 rows of the DataFrame\n",
    "\n",
    "loblolly.tail()    # display the last five rows of the DataFrame (the default)\n",
    "loblolly.tail(10)  # display the last 10 rows of the DataFrame\n",
    "loblolly.tail(50)  # display the last 50 rows of the DataFrame\n",
    "\n",
    "loblolly # display the first and last five rows of the DataFrame\n",
    "```\n",
    "\n",
    "Run the code cells below to inspect the data. Replace the `n` in `head()` and `tail()` with a number to specify how many rows should be displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab519c-91fb-4e88-92ce-9806289e3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly  # display the first and last five rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd373ba-d5a8-4808-868d-bc9cde6d2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.head(n)  # display the first n rows of the DataFrame (replace the n with desired number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af26f69-95c3-47d7-b497-bfeb415493ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.tail(n)  # display the last n rows of the DataFrame (replace the n with desired number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a32798-83b3-4e35-96c6-c59d38bf753a",
   "metadata": {},
   "source": [
    "#### Columns\n",
    "\n",
    "To lookup the names of the columns within a DataFrame, we can use the columns attribute. \n",
    "\n",
    "Run the code cell below to view the names of columns in the `loblolly` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e44b41-da2c-429e-a9c6-2e05dcb25374",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215de6f-c47b-4d71-9882-22692f4ca793",
   "metadata": {},
   "source": [
    "For a DataFrame object, we use a period to reference columns in Python (in R, we use the $ symbol). Specifically, we place a period between the DataFrame and the column name (e.g., `loblolly.height`).\n",
    "\n",
    "Run the code cell below. You will see that the head and tail of the column are displayed in addition to other column information: name, length, and data type. Modify the second code cell below to inspect a column other than height and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9ead5-808d-427d-a104-3338ff23b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.height  # inspect the height column in the loblolly DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df7a5a-54d5-4169-916d-4241c4f64d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.  # add a column name (other than height) after the period and run the cell to inspect the column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b0404-f9ae-485e-a1dd-41ce5ea664f2",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "\n",
    "Sometimes it is useful to select specific values within your data. Python uses an indexing system, like the majority of statistical packages, for both rows and columns. In Python, just like R, you must specify the value using the indices, or subscripts as they are sometimes called.\n",
    "\n",
    "The indices look like this `[r, c]` where *r* is the row value and *c* is the column value. \n",
    "\n",
    "The pandas module provides two different methods for indexing and selecting data:\n",
    "\n",
    "* Selection by position: a suite of methods to access data by integer-based indexing; use `.iloc`\n",
    "* Selection by label: a collection of methods to access data from a DataFrame by label-based indexing; use `.loc`\n",
    "\n",
    "##### Integer-based Indexing\n",
    "\n",
    "The following are considered valid inputs for accessing indexes for integer-based indexing:\n",
    "* Integers such as `3`, `60`, `99`\n",
    "* A list object or an array such as `[3, 60, 99]`\n",
    "* A slice object using integers `3:99`\n",
    "\n",
    "Recall the `loblolly` DataFrame contains three columns of data. If we wanted to access row 33, we would write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc2a58-4260-4c0d-976c-6207a886cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbdd97e-0a56-45b6-9166-18c0252458f4",
   "metadata": {},
   "source": [
    "Why didn't we use the number 33 if we wanted row 33? Remember, Python begins index values at 0, not 1. Thus, the first row has index value 0. For any row you want, subtract the value of 1 to obtain its index value such that `i = r - 1` where *i* is the index value you wish to obtain and *r* is the row you are attempting to access. For row 33, the index value is 32.\n",
    "\n",
    "What if we want only the first three rows? We could use slicing for this. The code cells below present two possible solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13649c3-e258-4635-8285-1cbf73dc934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f4ef3-b0a3-4687-ae63-ef34400fc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd92ba-421d-4218-a6c6-4d89233e4deb",
   "metadata": {},
   "source": [
    "Note, the values here are not inclusive. If we count up the number of index values input, four values should have been returned. Count them: 0, 1, 2, 3. That's four items. Yet, Python only returned 3 rows. Why is this? When using `.iloc` pandas is not inclusive when we perform a slice. For arrays, pandas is inclusive.\n",
    "\n",
    "Returning to our example, we can use an array as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adc2ef-69f4-4372-a291-04858e7eb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad670dd-1fb6-4068-afe6-ff494484c571",
   "metadata": {},
   "source": [
    "Now that we know how to access rows of data, what about columns of data? Simple. For integers, let's select row 3 and column 3. Just like rows, we need to subtract a value of 1 from the column number to obtain the index value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1d411-339b-4398-9a32-019472e6f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cbb9c-5e53-475d-b3c7-6d9e5e0da956",
   "metadata": {},
   "source": [
    "Using slicing as an input, let's select the first three rows and first two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e3512-01a7-4baa-9139-cae0828d1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[:3, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4c8a8-8f03-4e34-8686-4af3a270f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[0:3, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a268c4-c764-4d7b-a22a-3042820d0f6b",
   "metadata": {},
   "source": [
    "What about rows 10 through 16 and the last two columns? Sure, we can do that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877ea59-cd17-4fd4-a06a-72a921b152ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[9:16, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e0e8d-ef7a-40a4-a7a5-0dc0f87ebd95",
   "metadata": {},
   "source": [
    "Since we are using the technique slicing, we need to add one more value to the last integer in our slices. Instead of using `9:15` we used `9:16`; instead of `1:2` we use `1:3`.\n",
    "\n",
    "Another important note on slicing. If we would like all rows in the data but only the last two columns, we would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517c711-9663-4322-bb42-f17af79cc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[:, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b77d1-c7fb-493b-8c61-7c30aca4f9ea",
   "metadata": {},
   "source": [
    "Moving on to arrays, we can find the first three rows and last two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d4888-d72b-4358-a307-75729c76f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.iloc[[0, 1, 2], [1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfa3fe-9cd5-4703-a513-834e1ced109e",
   "metadata": {},
   "source": [
    "For a detailed review on integer-based indexing, [check out the pandas user guide](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-integer).\n",
    "\n",
    "##### Label-based Indexing\n",
    "\n",
    "The following are considered valid inputs for accessing indexes for label-based indexing.\n",
    "\n",
    "* Labels in the form of integers such as `3`, `60`, `90` or names of columns as strings.\n",
    "* A list or array of labels such as `['height', 'age', 'Seed']` for columns or `['a','b','c']` for rows if your index values are characters and not integers.\n",
    "* A slice object using labels `'height':'age'`\n",
    "\n",
    "Using slicing, we can access the columns in our DataFrame like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44d42b-5918-4f58-8ba9-b2f1487e6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.loc[:, 'age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5e588-55e6-4923-99bc-bceb15c3f928",
   "metadata": {},
   "source": [
    "This yields all rows of data with just the column `age`. If we would like to obtain all rows for the columns *height* through *age* we could use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577dc152-04f8-42b0-8e35-e2da85beaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.loc[:, 'height':'age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff3f49-93a0-4ab3-ae89-13a8855d4c82",
   "metadata": {},
   "source": [
    "What about rows? How will slicing work since the index values are numeric in nature? Well, `.loc` will treat them as if they were labels and not integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876885c-5fda-45a2-b633-e028885f616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.loc[0:3, 'height':'age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057fd70-bac5-47f2-8c95-9a445f0eddb4",
   "metadata": {},
   "source": [
    "This code pulls the first four records. Note, that this is *inclusive*, unlike `.iloc`. This is because the index values are not treated like integers; they are labels for all intents and purposes.\n",
    "\n",
    "Now that we understand slicing with labels, how about using arrays? Easy! Remember, we are dealing with labels now, not integers. This means when we select the columns, instead of referencing their index values as integers, we use their actual labels. For the first three rows and last two columns, the result would be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f4a7c-87d0-4876-9c5d-6fdf022cf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.loc[[0, 1, 2], ['age', 'Seed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011827b5-9d29-4ecc-9e90-dd3fe03a4346",
   "metadata": {},
   "source": [
    "For more details on label-based indexing, [check out the pandas user guide](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c646684-6f26-430b-8258-d851dd4ec1cd",
   "metadata": {},
   "source": [
    "#### Unique Values\n",
    "\n",
    "Another useful pandas function is `unique()`. This function identifies the unique values contained within your data. \n",
    "\n",
    "If you have categorical data, this can be helpful in determining the various values contained in a variable (in R, the term `factor` is used instead of categorical). Our loblolly DataFrame does not contain categorical data but we will use one of its columns here to demonstrate the code for the `unique()` function. To see a list of unique values in the `age` column, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f92eb-6a08-49e7-aa3f-dcfe849baf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(loblolly.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39db10f-1b8f-4b2c-b528-3e423721b72d",
   "metadata": {},
   "source": [
    "Now we know that `age` contains six unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161da83-b711-49be-9662-9e94cdc25fc0",
   "metadata": {},
   "source": [
    "#### Row and Column Metadata\n",
    "\n",
    "Another useful function, or set of functions, provides basic information on row and column size. This may be useful if we do not know the number of records that the data contains or the number of columns. \n",
    "\n",
    "Three methods are provided in the code cells below. \n",
    "\n",
    "* In the first code cell, `shape` provides information on rows and columns.\n",
    "* In the second and third code cell, the `len()` function merely assesses the length of the rows and columns.\n",
    "* In the fourth code cells, `info()` also provides information on rows and columns, including data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db38b30-2eb6-4c1b-bb77-9da0480f6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480f8ef-2150-4a65-bb92-a759ac7b34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loblolly.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6579c81-4459-4f11-8ede-acf9011b8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loblolly.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e3ba5-7d4e-4ffe-8091-2a0487cfed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6180ba-0f8e-4900-a1df-5c680856eda6",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "We can also inspect our DataFrame for missing values. The first method returns a TRUE-FALSE value based on whether it is complete; i.e. `TRUE` indicates no missing values whereas `FALSE` indicates missing values. This function is `notnull()`.\n",
    "\n",
    "```Python\n",
    "pd.notnull(loblolly)\n",
    "```\n",
    " \n",
    "To perform the opposite test, use the function `isnull()`. This returns TRUE for missing values and FALSE when no data missing.\n",
    "\n",
    "```Python\n",
    "pd.isnull(loblolly)\n",
    "```\n",
    " \n",
    "An important note: Datetime data types, specifically `datetime64[ns]`, `NaT` represents missing values, whereas `NaN` is typically used in numeric data types. Object data types will use the value provided them. pandas objects are intercompatible between NaT and NaN.\n",
    "\n",
    "### Adding Rows\n",
    "\n",
    "An important task we often engage in as we work with data in Python is appending new rows or adding new columns to a DataFrame. pandas provides a simple approach. \n",
    "\n",
    "First, create some new rows of data to populate the loblolly data. We are creating these rows as a new DataFrame so that we can easily combine it with the original loblolly DataFrame by using the `concat()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c461d82-a392-471d-abba-82bd48806380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some new old trees\n",
    "new_rows = pd.DataFrame({'height': [71.22, 85.05, 68.34], \n",
    "                         'age': [30, 30, 30],\n",
    "                         'Seed': [400, 401, 402]},\n",
    "                        index = [84, 85, 86]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9f5ea-018d-4c88-ae86-e130a0308bf7",
   "metadata": {},
   "source": [
    "Second, we can concatenate our new DataFrame to the existing loblolly DataFrame. Save this as a new DataFrame object named `loblolly_mod_1`. The reason we do this is to create a historical trail. If we need access to the original DataFrame, we cannot overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712e2c9-f0ab-425e-8987-7884587995e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge new trees with existing trees data\n",
    "loblolly_mod_1 = pd.concat([loblolly, new_rows], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8b78e-3125-4f42-8458-9e28135401fc",
   "metadata": {},
   "source": [
    "We can test to see if we successfully added the three new rows by looking at the lengths of the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4db87e-da63-4a00-af94-4385f959266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loblolly.index.values))  # original DataFrame\n",
    "\n",
    "print(len(loblolly_mod_1.index.values))  # new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1156a0a-4833-4225-b343-ac2b97816c05",
   "metadata": {},
   "source": [
    "We can also inspect the last three lines of the new DataFrame to see our new rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e32c3-7adc-4467-a0ae-df5f07da11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_1.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a08e90-6ffa-4301-a2f0-6005d22c3c5a",
   "metadata": {},
   "source": [
    "### Adding Columns \n",
    "\n",
    "Okay, we know how to add rows. How about adding columns? pandas has us covered. \n",
    "\n",
    "Say we want to create a new column called *growth* containing the annual growth rate of each tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3f224-44d4-44bf-a555-fc3adf3d9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = pd.DataFrame({'diameter': np.random.randint(1, 4, size = 87)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78cc30c-d63e-4d58-8d02-f858be807594",
   "metadata": {},
   "source": [
    "Using the `concat()` function, we can add the new column of data to the existing DataFrame. By default, `concat()` will stack DataFrames on top of each other (i.e., add new rows). We can tell pandas to place our DataFrames side by side (i.e., add new columns) with the `axis` parameter. Specifically, we set `axis` to `1` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702ed52-206f-4824-9e08-14e30f688458",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_2 = pd.concat([loblolly_mod_1, new_cols], axis = 1)\n",
    "loblolly_mod_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342fcd1-2016-4bcc-a43b-f6931ccee5b6",
   "metadata": {},
   "source": [
    "For more information on this topic, [check out the pandas user guide](http://pandas.pydata.org/pandas-docs/stable/merging.html).\n",
    "\n",
    "### Removing Columns\n",
    "\n",
    "Removing columns in Python is a straightforward process. After importing your data into a DataFrame, use the function `drop()` to remove the undesired columns. We will use the newly created `loblolly_mod_2` DataFrame for this section. To start, let's inspect the columns in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b63ea9-d12c-4ba7-b4a6-850c854b5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899837a-4cc5-4a19-b92d-e6bdb44ab0e8",
   "metadata": {},
   "source": [
    "In this example, we are not interested in the columns `Seed` or `diameter`. These columns are the third and fourth columns, respectively. To remove these columns, we would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c07ffd-e357-4896-a127-9c31e6ae385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_2.drop(['Seed', 'diameter'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9331c-aecb-4f30-8d62-b453bd003264",
   "metadata": {},
   "source": [
    "Note that the above code did not store or save our modification to the `loblolly_mod_2` DataFrame. Run the code cell below to check: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d316f8c-dfb5-4208-8fc2-43bbfc0f4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02173077-234a-4c43-aa52-446d29864290",
   "metadata": {},
   "source": [
    "The following code would save the DataFrame with `Seed` and `diameter` dropped:\n",
    "\n",
    "```Python\n",
    "loblolly_mod_2 = loblolly_mod_2.drop(['Seed', 'diameter'], axis = 1)\n",
    "```\n",
    "\n",
    "As an alternative method, you can use the column index values instead of the actual names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b6828-b0c2-4251-813d-ca6be38dc174",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_2.drop(loblolly_mod_2.columns[[2, 3]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d0be1-1ec1-4614-a176-b172eccec20b",
   "metadata": {},
   "source": [
    "The first example is a way to remove individual columns of data using the name of the column. If we have a long list of columns to remove, typing out individual column names would become tedious. The second example shows a way in which to remove columns using their index value. \n",
    "\n",
    "It should be noted that in Python, the indexing starts at 0 for both columns and rows; in R, indexing values start at 1. Thus, the fourth column in Python has an index value of 3, not 4.\n",
    "\n",
    "### Renaming Columns\n",
    "\n",
    "Renaming columns is simple. Python provides many possible ways to change a column header. The first requires that we type in all of the column names, even the ones we are not changing. This can be tedious if we have a lot of columns. If that is the case, the second method is better; also, it is recommended that we remove unwanted columns or create a subset of the data prior to renaming columns. In essence, we are renaming every single column in the DataFrame; you just are not changing some of them.\n",
    "\n",
    "For the first method, assume we would like to rename the column `diameter` to `trunk_diameter`. Our code for that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133b848-5859-4d11-b6db-d9e29c41ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_2.columns = ['height', 'age', 'Seed', 'trunk_diameter']\n",
    "loblolly_mod_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695dd9c2-9b88-4d8c-badb-661c8344427b",
   "metadata": {},
   "source": [
    "The second method uses the function `rename()`. This method will only allow the renaming of a single column of data. This avoids having to write out all the names of each column. Rename the column trunk_diameter back to diameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579354f4-2377-4a80-b6a9-55dc5ec5ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly_mod_2.rename(columns = {'trunk_diameter':'diameter'}, inplace = True)\n",
    "loblolly_mod_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f852aca-929f-4586-8f46-01d61f045733",
   "metadata": {},
   "source": [
    "### Sort Data\n",
    "\n",
    "We can sort data very easily. We can sort ascending, descending, select multiple columns to sort by, include only certain columns in results, and many other combinations. A simple sort would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f536a74-1704-4953-8ee6-44a84b5a1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.sort_values(by = 'height')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eec38b-e09b-41c3-bc99-e7c00a4c7e53",
   "metadata": {},
   "source": [
    "This sorts the data only on the column *height*. Note that if the data contained missing values, those would be listed toward the bottom of the sort. If we would like the missing values at the top, we would modify the code as follows:\n",
    "\n",
    "```Python\n",
    "loblolly.sort_values(by = 'height', na_position = 'first')\n",
    "```\n",
    "\n",
    "Sometimes we are only interested in the largest or smallest value within a column of data. Using the functions `nlargest()` and `nsmallest()`, we can obtain those values respectively. For example, say we want the six largest values for `height`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f817e-bdd2-4dfa-bc9f-bc8b18c62ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "loblolly.nlargest(6, 'height')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1e5b6-ca53-4588-b2cd-fa9a61befb04",
   "metadata": {},
   "source": [
    "Perhaps we would like the six largest values for both *height* and *age*. The code is not too much of an extension from the previous line:\n",
    "\n",
    "```Python\n",
    "loblolly.nlargest(6, ['height', 'age'])\n",
    "```\n",
    "\n",
    "Sorting multiple columns is also straight forward. Just add the additional columns in the order that you would like them sorted. If you want to sort by *height* and *age* in that order, you would list *height* first; if you want *age* sorted first, then list *age* first.\n",
    "\n",
    "```Python\n",
    "loblolly.sort_values(by = ['height', 'age'])  # sort by height then sort by age\n",
    "\n",
    "loblolly.sort_values(by = ['age', 'height'])  # sort by age then sort by height\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca6e4f-3ab0-4274-9c9c-f4a55198c694",
   "metadata": {},
   "source": [
    "### Sampling Data\n",
    "\n",
    "One basic approach to sampling is basing a subsample on a percentage of the overall sample size. For example, say we would like to sample 10% of the original data and perform an analysis on it. The steps include:\n",
    "* Determine how many rows is 10% of the data.\n",
    "* Find out how many rows are in the DataFrame.\n",
    "* Determine the range of the sample.\n",
    "* Perform the splitting.\n",
    "\n",
    "We will import a new dataset from the file diamonds.csv and use it to perform this operation. [This dataset is from R's ggplot2 package](https://ggplot2.tidyverse.org/reference/diamonds.html) and is provided in the data folder of this week's repo.\n",
    "\n",
    "Some example code for sampling is provided below. Notice the function `astype(int)`. The function `np.round()` returns a number with a decimal point, even when we request zero decimal points. To change the data type from float to an integer, we have to recast the object using `astype(int)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c8281-1150-4832-872a-aa2e70889259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "diamonds = pd.read_csv(\"diamonds.csv\")\n",
    "\n",
    "# quick inspect\n",
    "diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15946b52-a1b1-42ae-897a-7d3a77dbf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many rows is 10% of the data\n",
    "split_num = np.round((len(diamonds.index) * 0.1), 0).astype(int)\n",
    "split_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085bf9b-1f73-46c3-b16a-711bb24ac522",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_subsample = diamonds.sample(n = split_num, replace = False)  # sample by specifying the number of items to return\n",
    "len(diamonds_subsample.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9cd2e-7e0e-4db8-89d5-46977b6230b1",
   "metadata": {},
   "source": [
    "Another option is to skip the steps calculating the number of rows to sample. The function `sample()` provides the argument `frac` that allows you to specify the percentage you would like sampled. This is a quicker method than the previous one shown. Note that Both methods yield a length of 5,394 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f912779-f4b7-445e-aeb6-7f372fda70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_subsample = diamonds.sample(frac = 0.1, replace = False)  # sample by specifying the fraction of items to return\n",
    "len(diamonds_subsample.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b998bc-e254-4788-a613-3ac8f1752268",
   "metadata": {},
   "source": [
    "Often we will want to select a subsample based on certain conditions or criteria given the data. First, we read in the data (already done). Second, we convert a variable of interest to a categorical data type. We will use the `cut` column from the diamonds data for this example.\n",
    "\n",
    "Python uses the data type *object* by default. We can confirm this by using the following code to filter for just *object* data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ab939-4122-4e89-8d83-3d3057fe87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.select_dtypes(include = ['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96699de5-e1d3-41b2-9b09-07323c5dc6c4",
   "metadata": {},
   "source": [
    "This outputs three columns: `cut`, `color`, and `clarity`. The *object* data type is not the same as *category*. While both behave similarly, they are not equivalent. What is the difference? The reference page for pandas provides this description of *category*:\n",
    "\n",
    "> The categorical data type is useful in the following cases:\n",
    "> * A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory, see [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#categorical-memory).\n",
    "> * The lexical order of a variable is not the same as the logical order (\"one\", \"two\", \"three\"). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order, see [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#categorical-sort).\n",
    "> * As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types).\n",
    "\n",
    "If you have categorical data in a dataset, you should always convert it. In this case, we have three columns with categorical data that should be converted, but we will focus on one column, `cut`, to keep things simple.\n",
    "\n",
    "Okay, let's convert the data type for `cut` from *object* to *category*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74884bd0-9843-4165-ad19-7f7aaf2e5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds['cut'] = diamonds['cut'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46428b-3f63-4487-8d6d-dc9c2633ce02",
   "metadata": {},
   "source": [
    "*cut* contains five unique values as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a5cbf-bd7f-472d-bd2c-48f625e57e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.cut.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2018c-20ba-484c-8832-2b7b86839d8a",
   "metadata": {},
   "source": [
    "Let's assume we would like to perform an analysis only on diamonds with an *Ideal* cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df58e3-373c-40d1-afbc-277f9b3a11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[diamonds.cut == 'Ideal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf8ff8-2969-4c3f-a4f9-4292371ab25c",
   "metadata": {},
   "source": [
    "After perusing the *Ideal* diamond data, we realize that we only want data with a price value less than or equal to 400. This is another simple process. We just append additional conditions using the symbol `&`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602ce56-bd43-45e7-85bf-fd3769d31169",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[(diamonds.cut == 'Ideal') & (diamonds.price <= 400)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45298c-0ae1-4475-9229-555f60e99b69",
   "metadata": {},
   "source": [
    "What if we want to find data for diamonds that have an ideal or premium cut? \n",
    "\n",
    "We would type in both conditions and separate them using the OR operator, which is `|`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0599625-f765-4003-959a-27ed40445730",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[(diamonds.cut == 'Ideal') | (diamonds.cut == 'Premium')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f481bc7-39aa-4781-8c03-f93b3fbb2473",
   "metadata": {},
   "source": [
    "You may wish to create subsamples to perform techniques requiring training, testing, validation data or even k-fold cross validation data. For more information on using the scikit-learn and its functions, please review this webpage for [scikit-learn](http://scikit-learn.org/stable/modules/cross_validation.html#k-fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d72773-658f-4009-8baa-4affb0069e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 2)  # K-Fold cross-validator with 2 folds (the default is 5)\n",
    "\n",
    "for train, test in kf.split(diamonds):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d80916-639c-468f-b926-7611e2c06e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.iloc[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52120a8f-e4c6-4a5c-935c-6c81ee8e6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.iloc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e9997-4578-44c6-a9da-f7eb92c6f989",
   "metadata": {},
   "source": [
    "### Handling Dates and Times\n",
    "\n",
    "All modern statistical packages provide functions that perform mathematical operations and dates and times. \n",
    "\n",
    "Say we have data on employee work hours and we need to calculate pay for hourly employees. For each day over a five-day span we have the time the employee clocked in and the time the employee clocked out. We need to calculate the total number of hours the employee worked by using one of two methods: 1) we convert the date-time values into separate numerical values such as hours, minutes, and seconds and sum up the values or 2) use a date-time function that will automatically convert for us and provide you the total hours.\n",
    "\n",
    "The second option is the obvious choice as it requires minimal computational skills on our part. One of the downsides to most statistical packages is that they do not convert date-time values into date-time objects. That is, date-time values are read as categorical values made up of character strings; we cannot perform math on character strings. Converting date-time values in any statistical program requires work. \n",
    "\n",
    "Let's go over how to convert date-time values from *object* data types to *date-time* data types. We will use the data in the tennis_serve_time.csv file for this example. [This dataset is from the R fivethirtyeight package](https://vincentarelbundock.github.io/Rdatasets/doc/fivethirtyeight/tennis_serve_time.html) and is provided in the data folder of this week's repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e88988-0458-4dd4-8743-de3d50908834",
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis = pd.read_csv('tennis_serve_time.csv')  # read in tennis serve time data\n",
    "tennis.dtypes  # check data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2a015-cf40-4ac8-863b-675016b04c73",
   "metadata": {},
   "source": [
    "The process of converting to a date-time object is simpler in Python than it is in R because the pandas library provides powerful tools. \n",
    "\n",
    "We will focus on the *date* column for this example. Looking at the data itself reveals its string character formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c849400-00f2-4736-9c4f-afd93d8f0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis.date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f509587-3721-478a-a998-fbedb4db3334",
   "metadata": {},
   "source": [
    "To convert the column to a date-time format, we can use the `pd.to_datetime()` function. The data type is now `datetime64[ns]` instead of `object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb6108-2548-49b5-afac-80ec9092e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis['date'] = pd.to_datetime(tennis['date'])\n",
    "tennis.date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979751b-ca17-4c8b-9e38-c9107e857fde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "That wraps up this week's tutorial! Review this content as necessary.\n",
    "\n",
    "## Supplemental Materials: Data\n",
    "\n",
    "You may have noticed there are some datasets in the data folder that we did not interact with in the tutorial. I have included these data within the Week 7 GitHub repo in case you would like to practice and apply the methods described here to other datasets. Like the other datasets in this tutorial, most of these datasets are from R packages. The only exceptions are the data that you will use in your lab assignments:\n",
    "\n",
    "* github_teams.csv will be used in this week's *instructor-led* lab.\n",
    "  * Source: me (Olivia B. Newton)\n",
    "* CaliforniaHospitalData.csv and CaliforniaHospitalData_Personnel.txt will be used in this week's *independent* lab.\n",
    "  * Source: UM COB MIS professor Dr. Hammer\n",
    "\n",
    "If you are interested in practicing with more data beyond what is provided in this GitHub repo, [you can find and download a large variety of datasets associated with R packages on this handy website](https://vincentarelbundock.github.io/Rdatasets/index.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
